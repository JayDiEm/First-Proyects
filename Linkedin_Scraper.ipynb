{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4991ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b543187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2921621799469\n"
     ]
    }
   ],
   "source": [
    "# Add a start_time variable to measure the execution time\n",
    "start_time = time.time()\n",
    "#Create an empty list to store the job postings\n",
    "id_list = []\n",
    "start = 0  # Starting point\n",
    "results_per_page = 10 #How many results will the page show per search\n",
    "results_desired = 200  #How many results will the script save\n",
    "\n",
    "# Bucle while to reach the number of jobs desired\n",
    "\n",
    "while  len(id_list) < results_desired:\n",
    "    location = \"Argentina\"  # Job location\n",
    "   \n",
    "\n",
    "    # Construct the URL for LinkedIn job search\n",
    "    list_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=&location={location}&geoId=100446943&f_E=2%2C3%2C4&f_PP=103813819%2C117071347%2C102399085&f_TPR=&f_WT=3%2C2&start={start}\"\n",
    "\n",
    "\n",
    "    # Send a GET request to the URL and store the response\n",
    "    response = requests.get(list_url)\n",
    "\n",
    "    #Get the HTML, parse the response and find all list items(jobs postings)\n",
    "    list_data = response.text\n",
    "    list_soup = BeautifulSoup(list_data, \"html.parser\")\n",
    "    page_jobs = list_soup.find_all(\"li\")\n",
    "\n",
    " \n",
    "    #Itetrate through job postings to find job ids\n",
    "    for job in page_jobs:\n",
    "        try:\n",
    "            base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "            job_id = base_card_div.get(\"data-entity-urn\").split(\":\")[3]\n",
    "        except:\n",
    "            job_id = 0\n",
    "        id_list.append(job_id)\n",
    "        \n",
    "\n",
    "    start = start + results_per_page\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "time_spent = end_time - start_time\n",
    "\n",
    "print(time_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "901d1c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2f43c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296.40841126441956\n",
      "tu lista                                              job_title           company_name  \\\n",
      "0                 Agente de Contact Center Microcentro      BBVA en Argentina   \n",
      "1                                         ¡¿Te sumás?!        Galicia Seguros   \n",
      "2    Telefonista Administrativa (Estudiantes de Cs ...            Grupo Abans   \n",
      "3    ¡Sumate a nuestro equipo de Marketing Perfomance!               Despegar   \n",
      "4      ¡Sumate a nuestro equipo de Product Management!               Despegar   \n",
      "..                                                 ...                    ...   \n",
      "195                         Customer Success Team Lead  Automóviles San Jorge   \n",
      "196                                               None                   None   \n",
      "197                       Analista de Recursos Humanos    GLAM Distribuciones   \n",
      "198             Coordinador de Operaciones Comerciales       Laboratorio Elea   \n",
      "199                          Desarrollador/a Fullstack      BBVA en Argentina   \n",
      "\n",
      "      time_posted  \n",
      "0      1 week ago  \n",
      "1    3 months ago  \n",
      "2     4 weeks ago  \n",
      "3     3 weeks ago  \n",
      "4     3 weeks ago  \n",
      "..            ...  \n",
      "195    1 week ago  \n",
      "196          None  \n",
      "197  2 months ago  \n",
      "198  5 months ago  \n",
      "199   1 month ago  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# Initialize an empty list to store job information\n",
    "job_list = []\n",
    "\n",
    "# Loop through the list of job IDs and get each URL\n",
    "for job_id in id_list:\n",
    "    # Construct the URL for each job using the job ID\n",
    "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "    \n",
    "    # Send a GET request to the job URL and parse the reponse\n",
    "    job_response = requests.get(job_url)\n",
    "    job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "    \n",
    "     # Create a dictionary to store job details\n",
    "    job_post = {}\n",
    "    \n",
    "    # Try to extract and store the job title\n",
    "    try:\n",
    "        job_post[\"job_title\"] = job_soup.find(\"h2\", {\"class\":\"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"job_title\"] = None\n",
    "        \n",
    "    # Try to extract and store the company name\n",
    "    try:\n",
    "        job_post[\"company_name\"] = job_soup.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"company_name\"] = None\n",
    "        \n",
    "    # Try to extract and store the time posted\n",
    "    try:\n",
    "        job_post[\"time_posted\"] = job_soup.find(\"span\", {\"class\": \"posted-time-ago__text topcard__flavor--metadata\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"time_posted\"] = None\n",
    "        \n",
    "    # Try to extract and store the number of applicants\n",
    "#     try:\n",
    "#         job_post[\"num_applicants\"] = job_soup.find(\"span\", {\"class\": \"num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet\"}).text.strip()\n",
    "#     except:\n",
    "#         job_post[\"num_applicants\"] = None\n",
    "    \n",
    "        \n",
    "    # Append the job details to the job_list\n",
    "    job_list.append(job_post)\n",
    "\n",
    "# Create a pandas DataFrame using the list of job dictionaries 'job_list'\n",
    "jobs_df = pd.DataFrame(job_list)\n",
    "end_time = time.time()\n",
    "\n",
    "time_spent = end_time - start_time\n",
    "\n",
    "print(\"finished. Time spent:\", time_spent)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a501638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002999544143676758\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "jobs_df.to_csv('Linkeding_Scrapping3.csv', index = False)\n",
    "end_time = time.time()\n",
    "time_spent = end_time - start_time\n",
    "\n",
    "print(time_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb4700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
